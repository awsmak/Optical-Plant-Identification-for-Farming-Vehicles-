{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object detection using model maker","provenance":[],"collapsed_sections":[],"mount_file_id":"1RReKjlLkDqUmV9z8MuPQvPC8u-IIVnDo","authorship_tag":"ABX9TyPbmz9AbNnL6n1tsSlFNiaG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Install the required packages\n","Start by installing the required packages, including the Model Maker package from the GitHub repo and the pycocotools library you'll use for evaluation.\n","For avoiding \"PortAudio library not found\" error  import the sounddevice library in Google Colab.\n"],"metadata":{"id":"n5V1vrl0LwX5"}},{"cell_type":"code","source":["!pip install -q tflite-model-maker\n","!pip install -q tflite-support\n","!sudo apt-get install libportaudio2\n","!pip install sounddevice"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynHmL2NlMCQR","executionInfo":{"status":"ok","timestamp":1652727635788,"user_tz":-120,"elapsed":20376,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}},"outputId":"18bcd054-b955-45f9-9547-43d0c7bcc930"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'sudo apt autoremove' to remove them.\n","The following NEW packages will be installed:\n","  libportaudio2\n","0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n","Need to get 64.6 kB of archives.\n","After this operation, 215 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n","Fetched 64.6 kB in 0s (158 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libportaudio2:amd64.\n","(Reading database ... 155203 files and directories currently installed.)\n","Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n","Unpacking libportaudio2:amd64 (19.6.0-1) ...\n","Setting up libportaudio2:amd64 (19.6.0-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Requirement already satisfied: sounddevice in /usr/local/lib/python3.7/dist-packages (0.4.4)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.7/dist-packages (from sounddevice) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from CFFI>=1.0->sounddevice) (2.21)\n"]}]},{"cell_type":"markdown","source":["Import the required packages."],"metadata":{"id":"wrHmed2YMJZS"}},{"cell_type":"code","source":["import numpy as np\n","import os\n","\n","from tflite_model_maker.config import ExportFormat, QuantizationConfig\n","from tflite_model_maker import model_spec\n","from tflite_model_maker import object_detector\n","\n","from tflite_support import metadata\n","\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","\n","tf.get_logger().setLevel('ERROR')\n","from absl import logging\n","logging.set_verbosity(logging.ERROR)"],"metadata":{"id":"5u26TVFyMOYB","executionInfo":{"status":"ok","timestamp":1652727641593,"user_tz":-120,"elapsed":3240,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#Preparation of dataset\n","1. inside your google drive make a folder with name of the object to be detected(here rumex_acetosa).\n","2. Inside this folder create two more folders named train (containing train images with annotations) and validate (validation images with annotation).\n","3. mount the drive to the notebook and run the following code block."],"metadata":{"id":"tUGh71ivMwlC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hbcq9Q7RThF","executionInfo":{"status":"ok","timestamp":1652732041773,"user_tz":-120,"elapsed":11342,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}},"outputId":"87ceb536-7482-4718-b55a-88d69eef0299"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#Train the object detection model\n","###Step 1: Load the dataset\n","* Images in `train_data` is used to train the custom object detection model.\n","* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before.\n","* Provide the paths of tarin folder and validate folder as input directories\n","* Provide the label_map as label name (here rumex_acetosa)\n","\n"],"metadata":{"id":"-QpwAc5XS_MK"}},{"cell_type":"code","source":["train_data = object_detector.DataLoader.from_pascal_voc(images_dir= '/content/drive/MyDrive/rumex_acetosa_tfl/train',\n","                                                        annotations_dir='/content/drive/MyDrive/rumex_acetosa_tfl/train',\n","                                                        label_map = ['rumex_acetosa']\n","    \n",")\n","\n","val_data = object_detector.DataLoader.from_pascal_voc(images_dir= '/content/drive/MyDrive/rumex_acetosa_tfl/validate',\n","                                                        annotations_dir='/content/drive/MyDrive/rumex_acetosa_tfl/validate',\n","                                                        label_map = ['rumex_acetosa']\n","    \n",")\n","   "],"metadata":{"id":"XMfOJxpQTMG4","executionInfo":{"status":"ok","timestamp":1652729166026,"user_tz":-120,"elapsed":12002,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MRgcXc4gVYiA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 2: Select a model architecture\n","\n","EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n","\n","Here is the performance of each EfficientDet-Lite models compared to each others.\n","\n","| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n","|--------------------|-----------|---------------|----------------------|\n","| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n","| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n","| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n","| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n","| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n","\n","<i> * Size of the integer quantized models. <br/>\n","** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n","*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n","</i>\n","\n","In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."],"metadata":{"id":"bAUq3lLgVePU"}},{"cell_type":"code","source":["spec = model_spec.get('efficientdet_lite0')"],"metadata":{"id":"hJoA8t83Vja6","executionInfo":{"status":"ok","timestamp":1652729533840,"user_tz":-120,"elapsed":259,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Train the TensorFlow model with the training data.\n","\n","* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n","* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n","* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model.\n","* You can change the hyper parameters according to your dataset and requirements"],"metadata":{"id":"hlF1st9XW50b"}},{"cell_type":"code","source":["model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtm2TBKnW7DY","executionInfo":{"status":"ok","timestamp":1652731415051,"user_tz":-120,"elapsed":1426523,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}},"outputId":"53782b1b-1ed6-4507-a1de-5f8eee435bc7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","19/19 [==============================] - 113s 4s/step - det_loss: 1.7049 - cls_loss: 1.0913 - box_loss: 0.0123 - reg_l2_loss: 0.0630 - loss: 1.7679 - learning_rate: 0.0065 - gradient_norm: 2.3159 - val_det_loss: 1.5321 - val_cls_loss: 1.0227 - val_box_loss: 0.0102 - val_reg_l2_loss: 0.0630 - val_loss: 1.5951\n","Epoch 2/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 1.4944 - cls_loss: 0.9181 - box_loss: 0.0115 - reg_l2_loss: 0.0630 - loss: 1.5574 - learning_rate: 0.0049 - gradient_norm: 3.0266 - val_det_loss: 1.1727 - val_cls_loss: 0.6984 - val_box_loss: 0.0095 - val_reg_l2_loss: 0.0630 - val_loss: 1.2357\n","Epoch 3/20\n","19/19 [==============================] - 68s 4s/step - det_loss: 1.1305 - cls_loss: 0.6169 - box_loss: 0.0103 - reg_l2_loss: 0.0630 - loss: 1.1935 - learning_rate: 0.0048 - gradient_norm: 4.9346 - val_det_loss: 1.0625 - val_cls_loss: 0.6136 - val_box_loss: 0.0090 - val_reg_l2_loss: 0.0630 - val_loss: 1.1255\n","Epoch 4/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 1.0413 - cls_loss: 0.5317 - box_loss: 0.0102 - reg_l2_loss: 0.0630 - loss: 1.1043 - learning_rate: 0.0046 - gradient_norm: 4.7594 - val_det_loss: 0.9071 - val_cls_loss: 0.4888 - val_box_loss: 0.0084 - val_reg_l2_loss: 0.0630 - val_loss: 0.9701\n","Epoch 5/20\n","19/19 [==============================] - 76s 4s/step - det_loss: 0.9477 - cls_loss: 0.4818 - box_loss: 0.0093 - reg_l2_loss: 0.0631 - loss: 1.0108 - learning_rate: 0.0043 - gradient_norm: 5.1775 - val_det_loss: 0.8463 - val_cls_loss: 0.4458 - val_box_loss: 0.0080 - val_reg_l2_loss: 0.0631 - val_loss: 0.9093\n","Epoch 6/20\n","19/19 [==============================] - 64s 3s/step - det_loss: 0.8543 - cls_loss: 0.4271 - box_loss: 0.0085 - reg_l2_loss: 0.0631 - loss: 0.9174 - learning_rate: 0.0040 - gradient_norm: 4.9358 - val_det_loss: 0.8236 - val_cls_loss: 0.4184 - val_box_loss: 0.0081 - val_reg_l2_loss: 0.0631 - val_loss: 0.8867\n","Epoch 7/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 0.8216 - cls_loss: 0.3921 - box_loss: 0.0086 - reg_l2_loss: 0.0631 - loss: 0.8847 - learning_rate: 0.0037 - gradient_norm: 4.2362 - val_det_loss: 0.8073 - val_cls_loss: 0.4168 - val_box_loss: 0.0078 - val_reg_l2_loss: 0.0631 - val_loss: 0.8704\n","Epoch 8/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 0.7542 - cls_loss: 0.3688 - box_loss: 0.0077 - reg_l2_loss: 0.0631 - loss: 0.8172 - learning_rate: 0.0033 - gradient_norm: 3.6127 - val_det_loss: 0.8280 - val_cls_loss: 0.4415 - val_box_loss: 0.0077 - val_reg_l2_loss: 0.0631 - val_loss: 0.8911\n","Epoch 9/20\n","19/19 [==============================] - 64s 3s/step - det_loss: 0.7673 - cls_loss: 0.3877 - box_loss: 0.0076 - reg_l2_loss: 0.0631 - loss: 0.8304 - learning_rate: 0.0029 - gradient_norm: 4.3046 - val_det_loss: 0.6950 - val_cls_loss: 0.3451 - val_box_loss: 0.0070 - val_reg_l2_loss: 0.0631 - val_loss: 0.7581\n","Epoch 10/20\n","19/19 [==============================] - 67s 4s/step - det_loss: 0.6938 - cls_loss: 0.3406 - box_loss: 0.0071 - reg_l2_loss: 0.0631 - loss: 0.7570 - learning_rate: 0.0025 - gradient_norm: 3.7014 - val_det_loss: 0.6669 - val_cls_loss: 0.3441 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0631 - val_loss: 0.7300\n","Epoch 11/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 0.7045 - cls_loss: 0.3512 - box_loss: 0.0071 - reg_l2_loss: 0.0631 - loss: 0.7676 - learning_rate: 0.0021 - gradient_norm: 4.5427 - val_det_loss: 0.7000 - val_cls_loss: 0.3864 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0631 - val_loss: 0.7631\n","Epoch 12/20\n","19/19 [==============================] - 68s 4s/step - det_loss: 0.6622 - cls_loss: 0.3332 - box_loss: 0.0066 - reg_l2_loss: 0.0631 - loss: 0.7253 - learning_rate: 0.0017 - gradient_norm: 4.2222 - val_det_loss: 0.6744 - val_cls_loss: 0.3375 - val_box_loss: 0.0067 - val_reg_l2_loss: 0.0631 - val_loss: 0.7375\n","Epoch 13/20\n","19/19 [==============================] - 64s 3s/step - det_loss: 0.6942 - cls_loss: 0.3579 - box_loss: 0.0067 - reg_l2_loss: 0.0631 - loss: 0.7573 - learning_rate: 0.0013 - gradient_norm: 4.7609 - val_det_loss: 0.5969 - val_cls_loss: 0.3073 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.0631 - val_loss: 0.6600\n","Epoch 14/20\n","19/19 [==============================] - 67s 3s/step - det_loss: 0.6412 - cls_loss: 0.3316 - box_loss: 0.0062 - reg_l2_loss: 0.0631 - loss: 0.7043 - learning_rate: 9.6720e-04 - gradient_norm: 4.8184 - val_det_loss: 0.6585 - val_cls_loss: 0.3623 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0631 - val_loss: 0.7217\n","Epoch 15/20\n","19/19 [==============================] - 67s 4s/step - det_loss: 0.6249 - cls_loss: 0.3305 - box_loss: 0.0059 - reg_l2_loss: 0.0631 - loss: 0.6880 - learning_rate: 6.6368e-04 - gradient_norm: 4.4672 - val_det_loss: 0.6475 - val_cls_loss: 0.3608 - val_box_loss: 0.0057 - val_reg_l2_loss: 0.0631 - val_loss: 0.7106\n","Epoch 16/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 0.6333 - cls_loss: 0.3371 - box_loss: 0.0059 - reg_l2_loss: 0.0631 - loss: 0.6964 - learning_rate: 4.1024e-04 - gradient_norm: 4.6282 - val_det_loss: 0.6758 - val_cls_loss: 0.3820 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0631 - val_loss: 0.7390\n","Epoch 17/20\n","19/19 [==============================] - 65s 3s/step - det_loss: 0.6615 - cls_loss: 0.3504 - box_loss: 0.0062 - reg_l2_loss: 0.0631 - loss: 0.7247 - learning_rate: 2.1381e-04 - gradient_norm: 5.2209 - val_det_loss: 0.6758 - val_cls_loss: 0.3864 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.0631 - val_loss: 0.7389\n","Epoch 18/20\n","19/19 [==============================] - 64s 3s/step - det_loss: 0.5994 - cls_loss: 0.3155 - box_loss: 0.0057 - reg_l2_loss: 0.0631 - loss: 0.6625 - learning_rate: 7.9733e-05 - gradient_norm: 4.6615 - val_det_loss: 0.6618 - val_cls_loss: 0.3768 - val_box_loss: 0.0057 - val_reg_l2_loss: 0.0631 - val_loss: 0.7249\n","Epoch 19/20\n","19/19 [==============================] - 64s 3s/step - det_loss: 0.6193 - cls_loss: 0.3281 - box_loss: 0.0058 - reg_l2_loss: 0.0631 - loss: 0.6825 - learning_rate: 1.1679e-05 - gradient_norm: 3.9960 - val_det_loss: 0.6603 - val_cls_loss: 0.3749 - val_box_loss: 0.0057 - val_reg_l2_loss: 0.0631 - val_loss: 0.7234\n","Epoch 20/20\n","19/19 [==============================] - 67s 4s/step - det_loss: 0.6500 - cls_loss: 0.3364 - box_loss: 0.0063 - reg_l2_loss: 0.0631 - loss: 0.7131 - learning_rate: 1.1500e-05 - gradient_norm: 4.6230 - val_det_loss: 0.6569 - val_cls_loss: 0.3726 - val_box_loss: 0.0057 - val_reg_l2_loss: 0.0631 - val_loss: 0.7200\n"]}]},{"cell_type":"markdown","source":["### Step 4. Evaluate the model with the validation data.\n","\n","After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n","\n","As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n","\n","The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."],"metadata":{"id":"iYRsAyQxXftl"}},{"cell_type":"code","source":["model.evaluate(val_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jo4ZbmnAXj4p","executionInfo":{"status":"ok","timestamp":1652731448621,"user_tz":-120,"elapsed":6786,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}},"outputId":"e6b7ec5b-e409-4d63-c6b4-c968720ff750"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\r1/1 [==============================] - 5s 5s/step\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["{'AP': 0.2664923,\n"," 'AP50': 0.5398595,\n"," 'AP75': 0.23577946,\n"," 'AP_/rumex_acetosa': 0.2664923,\n"," 'APl': 0.2664923,\n"," 'APm': -1.0,\n"," 'APs': -1.0,\n"," 'ARl': 0.45384616,\n"," 'ARm': -1.0,\n"," 'ARmax1': 0.13846155,\n"," 'ARmax10': 0.41538462,\n"," 'ARmax100': 0.45384616,\n"," 'ARs': -1.0}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### Step 5: Export as a TensorFlow Lite model.\n","\n","Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU.\n","\n","\n","1.   Create a new folder named model in the parent folder inside the drive\n","2.   Provide the path for this folder as export_dir below.\n","\n"],"metadata":{"id":"X-PcgNGlX8md"}},{"cell_type":"code","source":["model.export(export_dir='/content/drive/MyDrive/rumex_acetosa_tfl/model', tflite_filename='rumex_acetosa.tflite')"],"metadata":{"id":"EmypHZg4X_xo","executionInfo":{"status":"ok","timestamp":1652731614146,"user_tz":-120,"elapsed":148666,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["###Step 6: Evaluate the TensorFlow Lite model.\n","Several factors can affect the model accuracy when exporting to TFLite:\n","\n","\n","\n","*   Quantization helps shrinking the model size by 4 times at the expense of some accuracy drop.\n","*   The original TensorFlow model uses per-class non-max supression (NMS) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate. Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n","\n","Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model.\n","\n"],"metadata":{"id":"dj9PAik2YHkM"}},{"cell_type":"code","source":["model.evaluate_tflite('/content/drive/MyDrive/rumex_acetosa_tfl/model/rumex_acetosa.tflite', val_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFdPk6vgfToI","executionInfo":{"status":"ok","timestamp":1652731992440,"user_tz":-120,"elapsed":22411,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}},"outputId":"a758c7b8-5357-465f-8664-d7c781b31061"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 13s 3s/step\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["{'AP': 0.24951378,\n"," 'AP50': 0.585635,\n"," 'AP75': 0.25314835,\n"," 'AP_/rumex_acetosa': 0.24951378,\n"," 'APl': 0.24951378,\n"," 'APm': -1.0,\n"," 'APs': -1.0,\n"," 'ARl': 0.42307693,\n"," 'ARm': -1.0,\n"," 'ARmax1': 0.13846155,\n"," 'ARmax10': 0.3923077,\n"," 'ARmax100': 0.42307693,\n"," 'ARs': -1.0}"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["###Step 7: Download the TFLite model to your local computer\n","This file contains the weights and can be used to implement object dtection on raspberry pi"],"metadata":{"id":"fzKoBzaEfuq7"}},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/drive/MyDrive/rumex_acetosa_tfl/model/rumex_acetosa.tflite')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"LB5R-0WjYd2H","executionInfo":{"status":"ok","timestamp":1652732013996,"user_tz":-120,"elapsed":240,"user":{"displayName":"Akash Ashokan","userId":"15605972753183474440"}},"outputId":"b451d3b9-8c9e-4eef-89a9-2754a6340e01"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ed56bc92-66a8-442f-a7ad-c6d9d6dbb78b\", \"rumex_acetosa.tflite\", 4444724)"]},"metadata":{}}]}]}