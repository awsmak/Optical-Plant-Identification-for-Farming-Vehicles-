{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/akash/My_Projects/Optical-Plant-Identification-for-Farming-Vehicles-/darknet\n",
      "3rdparty\t\tdarknet_video.py       net_cam_v4.sh\n",
      "backup\t\t\tdata\t\t       obj\n",
      "bad.list\t\tdocker-compose.yml     package.xml\n",
      "build\t\t\tDockerfile.cpu\t       README.md\n",
      "build.ps1\t\tDockerfile.gpu\t       results\n",
      "cfg\t\t\timage_yolov3.sh        scripts\n",
      "cmake\t\t\timage_yolov4.sh        src\n",
      "CMakeLists.txt\t\tinclude\t\t       vcpkg.json\n",
      "darknet\t\t\tjson_mjpeg_streams.sh  video_yolov3.sh\n",
      "DarknetConfig.cmake.in\tLICENSE\t\t       video_yolov4.sh\n",
      "darknet_images.py\tMakefile\t       yolov3-tiny.weights\n",
      "darknet.py\t\tnet_cam_v3.sh\t       yolov3-tiny.weights.1\n",
      "usage: ./darknet <function>\n",
      "Changed back to notebook directory: /home/akash/My_Projects/Optical-Plant-Identification-for-Farming-Vehicles-/darknet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the Darknet directory to the system path\n",
    "darknet_path = os.path.abspath('../darknet')\n",
    "if darknet_path not in sys.path:\n",
    "    sys.path.append(darknet_path)\n",
    "\n",
    "# Change the working directory to the Darknet directory\n",
    "os.chdir(darknet_path)\n",
    "\n",
    "# Verify the current working directory and Darknet installation\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "!ls  # List contents of Darknet directory\n",
    "!./darknet  # Run Darknet to see usage information\n",
    "\n",
    "# Change back to the notebook's directory\n",
    "os.chdir(os.path.dirname(os.path.abspath('__file__')))\n",
    "print(f\"Changed back to notebook directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-17 16:30:59--  https://pjreddie.com/media/files/yolov3-tiny.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
      "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35434956 (34M) [application/octet-stream]\n",
      "Saving to: ‘yolov3-tiny.weights.2’\n",
      "\n",
      "yolov3-tiny.weights 100%[===================>]  33.79M  3.37MB/s    in 16s     \n",
      "\n",
      "2024-10-17 16:31:59 (2.13 MB/s) - ‘yolov3-tiny.weights.2’ saved [35434956/35434956]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained weights for the convolutional layers\n",
    "!wget https://pjreddie.com/media/files/yolov3-tiny.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of data/rumex.data:\n",
      "classes = 1\n",
      "train = data/train.txt\n",
      "valid = data/valid.txt\n",
      "names = data/rumex.names\n",
      "backup = backup/\n",
      "Contents of data/rumex.names:\n",
      "rumex_acetosa\n"
     ]
    }
   ],
   "source": [
    "# Create necessary configuration files\n",
    "with open('data/rumex.data', 'w') as f:\n",
    "    f.write(f\"classes = 1\\n\")\n",
    "    f.write(f\"train = data/train.txt\\n\")\n",
    "    f.write(f\"valid = data/valid.txt\\n\")\n",
    "    f.write(f\"names = data/rumex.names\\n\")\n",
    "    f.write(f\"backup = backup/\\n\")\n",
    "\n",
    "print(\"Contents of data/rumex.data:\")\n",
    "!cat data/rumex.data\n",
    "\n",
    "with open('data/rumex.names', 'w') as f:\n",
    "    f.write(\"rumex_acetosa\\n\")\n",
    "\n",
    "print(\"Contents of data/rumex.names:\")\n",
    "!cat data/rumex.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ../darknet/data/train.txt with 250 entries\n",
      "Created ../darknet/data/valid.txt with 15 entries\n",
      "First few lines of data/train.txt:\n",
      "../data/augmented_dataset/train/SAM_1883_JPG.rf.6ac03c86430f7077b6ad7770375674a4_aug_81d96be44e584d2e975d0f57405f32c7.jpg\n",
      "../data/augmented_dataset/train/SAM_1947_JPG.rf.ab793c5998755603f8ae666cf1e46aea_aug_e6708d235a21461b8cc957dca7bbb947.jpg\n",
      "../data/augmented_dataset/train/SAM_1912_JPG.rf.e4d8dea398b6cc5bbd5bd4e21104bde4_aug_de53ef651cb34edcb08ce06839e79c0a.jpg\n",
      "../data/augmented_dataset/train/SAM_1893_JPG.rf.c68d42fee12c9edaa49220c27462d40d_aug_2f9e5ca20efa46caa4c455eb6a8da1a6.jpg\n",
      "../data/augmented_dataset/train/SAM_1880_JPG.rf.dfc7414e9efb61f33bbd7d70e965a246_aug_c9055c10e35e47249ba18b9d0f8109a5.jpg\n",
      "First few lines of data/valid.txt:\n",
      "../data/augmented_dataset/valid/SAM_1895_JPG.rf.34ffcfdc4d452b1d4445cf9584eb58d7.jpg\n",
      "../data/augmented_dataset/valid/SAM_1921_JPG.rf.2266e7416f743c397fe91f5f108bb462.jpg\n",
      "../data/augmented_dataset/valid/SAM_1940_JPG.rf.c2f3e3b28914c9c7d569e02d82f90d61.jpg\n",
      "../data/augmented_dataset/valid/SAM_1909_JPG.rf.1de8e0dda6773a35820b4912c53b4a9a.jpg\n",
      "../data/augmented_dataset/valid/SAM_1894_JPG.rf.0a4039f63042ec889e1e2192a0499576.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def create_list(path, output_file):\n",
    "    darknet_dir = \"/home/akash/My_Projects/Optical-Plant-Identification-for-Farming-Vehicles-/darknet\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "            # Create path relative to darknet directory\n",
    "            relative_path = os.path.relpath(img_path, start=darknet_dir)\n",
    "            f.write(relative_path + '\\n')\n",
    "    print(f\"Created {output_file} with {len(glob.glob(os.path.join(path, '*.jpg')))} entries\")\n",
    "\n",
    "# Paths relative to the script location\n",
    "train_path = '../data/augmented_dataset/train'\n",
    "valid_path = '../data/augmented_dataset/valid'\n",
    "train_output = '../darknet/data/train.txt'\n",
    "valid_output = '../darknet/data/valid.txt'\n",
    "\n",
    "create_list(train_path, train_output)\n",
    "create_list(valid_path, valid_output)\n",
    "\n",
    "print(\"First few lines of data/train.txt:\")\n",
    "!head -n 5 data/train.txt\n",
    "\n",
    "print(\"First few lines of data/valid.txt:\")\n",
    "!head -n 5 data/valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of cfg/yolov3-tiny-rumex.cfg:\n",
      "[net]\n",
      "# Testing\n",
      "batch=16\n",
      "subdivisions=4\n",
      "# Training\n",
      "# batch=64\n",
      "# subdivisions=2\n",
      "width=416\n",
      "height=416\n",
      "channels=3\n",
      "momentum=0.9\n",
      "decay=0.0005\n",
      "angle=0\n",
      "saturation = 1.5\n",
      "exposure = 1.5\n",
      "hue=.1\n",
      "\n",
      "learning_rate=0.001\n",
      "burn_in=1000\n",
      "max_batches = 4000\n",
      "policy=steps\n",
      "steps=3200,3600\n",
      "scales=.1,.1\n",
      "\n",
      "[convolutional]\n",
      "batch_normalize=1\n",
      "filters=16\n",
      "size=3\n",
      "stride=1\n",
      "pad=1\n"
     ]
    }
   ],
   "source": [
    "# Modify yolov3-tiny configuration\n",
    "with open('cfg/yolov3-tiny.cfg', 'r') as f:\n",
    "    config = f.read()\n",
    "\n",
    "# Adjust batch and subdivisions\n",
    "config = config.replace('batch=1', 'batch=16')\n",
    "config = config.replace('subdivisions=1', 'subdivisions=4')\n",
    "\n",
    "# Adjust for single class\n",
    "config = config.replace('classes=80', 'classes=1')\n",
    "\n",
    "# Adjust filters for single class (3 * (5 + classes))\n",
    "config = config.replace('filters=255', 'filters=18', 2)  # Replace only the last two occurrences\n",
    "\n",
    "# Adjust max_batches, steps, and learning rate\n",
    "lines = config.split('\\n')\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith('max_batches'):\n",
    "        lines[i] = 'max_batches = 4000'  # 2000 * number of classes, but minimum 4000\n",
    "    elif line.startswith('steps'):\n",
    "        lines[i] = 'steps=3200,3600'  # 80% and 90% of max_batches\n",
    "    elif line.startswith('learning_rate'):\n",
    "        lines[i] = 'learning_rate=0.001'  # Reduced learning rate for fine-tuning\n",
    "\n",
    "# Add data augmentation options\n",
    "lines.insert(3, 'angle=0')\n",
    "lines.insert(4, 'saturation = 1.5')\n",
    "lines.insert(5, 'exposure = 1.5')\n",
    "lines.insert(6, 'hue = .1')\n",
    "\n",
    "config = '\\n'.join(lines)\n",
    "\n",
    "with open('cfg/yolov3-tiny-rumex.cfg', 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(\"Contents of cfg/yolov3-tiny-rumex.cfg:\")\n",
    "!head -n 30 cfg/yolov3-tiny-rumex.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started. Output:\n",
      " CUDA-version: 12000 (12040), cuDNN: 9.4.0, GPU count: 1  \n",
      " OpenCV version: 4.6.0\n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: NVIDIA GeForce GTX 1660 Ti \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  16 0.150 BF\n",
      "   1 max                2x 2/ 2    416 x 416 x  16 ->  208 x 208 x  16 0.003 BF\n",
      "   2 conv     32       3 x 3/ 1    208 x 208 x  16 ->  208 x 208 x  32 0.399 BF\n",
      "   3 max                2x 2/ 2    208 x 208 x  32 ->  104 x 104 x  32 0.001 BF\n",
      "   4 conv     64       3 x 3/ 1    104 x 104 x  32 ->  104 x 104 x  64 0.399 BF\n",
      "   5 max                2x 2/ 2    104 x 104 x  64 ->   52 x  52 x  64 0.001 BF\n",
      "   6 conv    128       3 x 3/ 1     52 x  52 x  64 ->   52 x  52 x 128 0.399 BF\n",
      "   7 max                2x 2/ 2     52 x  52 x 128 ->   26 x  26 x 128 0.000 BF\n",
      "   8 conv    256       3 x 3/ 1     26 x  26 x 128 ->   26 x  26 x 256 0.399 BF\n",
      "   9 max                2x 2/ 2     26 x  26 x 256 ->   13 x  13 x 256 0.000 BF\n",
      "  10 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF\n",
      "  11 max                2x 2/ 1     13 x  13 x 512 ->   13 x  13 x 512 0.000 BF\n",
      "  12 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
      "  13 conv    256       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 256 0.089 BF\n",
      "  14 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF\n",
      "  15 conv     18       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x  18 0.003 BF\n",
      "  16 yolo\n",
      "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      "  17 route  13 \t\t                           ->   13 x  13 x 256 \n",
      "  18 conv    128       1 x 1/ 1     13 x  13 x 256 ->   13 x  13 x 128 0.011 BF\n",
      "  19 upsample                 2x    13 x  13 x 128 ->   26 x  26 x 128\n",
      "  20 route  19 8 \t                           ->   26 x  26 x 384 \n",
      "  21 conv    256       3 x 3/ 1     26 x  26 x 384 ->   26 x  26 x 256 1.196 BF\n",
      "  22 conv     18       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x  18 0.006 BF\n",
      "  23 yolo\n",
      "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      "Total BFLOPS 5.448 \n",
      "avg_outputs = 324846 \n",
      " Allocate additional workspace_size = 52.44 MB \n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: NVIDIA GeForce GTX 1660 Ti \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  16 0.150 BF\n",
      "   1 max                2x 2/ 2    416 x 416 x  16 ->  208 x 208 x  16 0.003 BF\n",
      "   2 conv     32       3 x 3/ 1    208 x 208 x  16 ->  208 x 208 x  32 0.399 BF\n",
      "   3 max                2x 2/ 2    208 x 208 x  32 ->  104 x 104 x  32 0.001 BF\n",
      "   4 conv     64       3 x 3/ 1    104 x 104 x  32 ->  104 x 104 x  64 0.399 BF\n",
      "   5 max                2x 2/ 2    104 x 104 x  64 ->   52 x  52 x  64 0.001 BF\n",
      "   6 conv    128       3 x 3/ 1     52 x  52 x  64 ->   52 x  52 x 128 0.399 BF\n",
      "   7 max                2x 2/ 2     52 x  52 x 128 ->   26 x  26 x 128 0.000 BF\n",
      "   8 conv    256       3 x 3/ 1     26 x  26 x 128 ->   26 x  26 x 256 0.399 BF\n",
      "   9 max                2x 2/ 2     26 x  26 x 256 ->   13 x  13 x 256 0.000 BF\n",
      "  10 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF\n",
      "  11 max                2x 2/ 1     13 x  13 x 512 ->   13 x  13 x 512 0.000 BF\n",
      "  12 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
      "  13 conv    256       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 256 0.089 BF\n",
      "  14 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF\n",
      "  15 conv     18       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x  18 0.003 BF\n",
      "  16 yolo\n",
      "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      "  17 route  13 \t\t                           ->   13 x  13 x 256 \n",
      "  18 conv    128       1 x 1/ 1     13 x  13 x 256 ->   13 x  13 x 128 0.011 BF\n",
      "  19 upsample                 2x    13 x  13 x 128 ->   26 x  26 x 128\n",
      "  20 route  19 8 \t                           ->   26 x  26 x 384 \n",
      "  21 conv    256       3 x 3/ 1     26 x  26 x 384 ->   26 x  26 x 256 1.196 BF\n",
      "  22 conv     18       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x  18 0.006 BF\n",
      "  23 yolo\n",
      "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      "Total BFLOPS 5.448 \n",
      "avg_outputs = 324846 \n",
      " Allocate additional workspace_size = 52.44 MB \n",
      "Loading weights from yolov3-tiny.weights... Prepare additional network for mAP calculation...\n",
      "net.optimized_memory = 0 \n",
      "mini_batch = 1, batch = 4, time_steps = 1, train = 0 \n",
      "Create CUDA-stream - 0 \n",
      " Create cudnn-handle 0 \n",
      "yolov3-tiny-rumex\n",
      "net.optimized_memory = 0 \n",
      "mini_batch = 4, batch = 16, time_steps = 1, train = 1 \n",
      "Done! Loaded 24 layers from weights-file \n",
      "saveweights: Using default '1000'\n",
      "savelast: Using default '100'\n",
      "Saving weights to backup//yolov3-tiny-rumex_final.weights\n",
      " Create 6 permanent cpu-threads \n",
      "\n",
      " seen 64, trained: 32013 K-images (500 Kilo-batches_64) \n",
      "Weights are saved after: 1000 iterations. Last weights (*_last.weight) are stored every 100 iterations. \n",
      "Learning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\n",
      " Detection layer: 16 - type = 28 \n",
      " Detection layer: 23 - type = 28 \n",
      "If you want to train from the beginning, then use flag in the end of training command: -clear \n",
      "Training process finished with return code: 0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Start training with transfer learning\n",
    "try:\n",
    "    process = subprocess.Popen(\n",
    "        ['./darknet', 'detector', 'train', \n",
    "         'data/rumex.data', \n",
    "         'cfg/yolov3-tiny-rumex.cfg', \n",
    "         'yolov3-tiny.weights',  # Use the pre-trained weights\n",
    "         '-map',  # Calculate mAP during training\n",
    "         '-dont_show'  # Don't show images during training (useful for remote servers)\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "\n",
    "    print(\"Training started. Output:\")\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "        sys.stdout.flush()  # Ensure output is displayed immediately\n",
    "\n",
    "    process.wait()\n",
    "    print(\"Training process finished with return code:\", process.returncode)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of train.txt:\n",
      "../data/augmented_dataset/train/SAM_1883_JPG.rf.6ac03c86430f7077b6ad7770375674a4_aug_81d96be44e584d2e975d0f57405f32c7.jpg\n",
      "../data/augmented_dataset/train/SAM_1947_JPG.rf.ab793c5998755603f8ae666cf1e46aea_aug_e6708d235a21461b8cc957dca7bbb947.jpg\n",
      "../data/augmented_dataset/train/SAM_1912_JPG.rf.e4d8dea398b6cc5bbd5bd4e21104bde4_aug_de53ef651cb34edcb08ce06839e79c0a.jpg\n",
      "../data/augmented_dataset/train/SAM_1893_JPG.rf.c68d42fee12c9edaa49220c27462d40d_aug_2f9e5ca20efa46caa4c455eb6a8da1a6.jpg\n",
      "../data/augmented_dataset/train/SAM_1880_JPG.rf.dfc7414e9efb61f33bbd7d70e965a246_aug_c9055c10e35e47249ba18b9d0f8109a5.jpg\n",
      "\n",
      "Contents of valid.txt:\n",
      "../data/augmented_dataset/valid/SAM_1895_JPG.rf.34ffcfdc4d452b1d4445cf9584eb58d7.jpg\n",
      "../data/augmented_dataset/valid/SAM_1921_JPG.rf.2266e7416f743c397fe91f5f108bb462.jpg\n",
      "../data/augmented_dataset/valid/SAM_1940_JPG.rf.c2f3e3b28914c9c7d569e02d82f90d61.jpg\n",
      "../data/augmented_dataset/valid/SAM_1909_JPG.rf.1de8e0dda6773a35820b4912c53b4a9a.jpg\n",
      "../data/augmented_dataset/valid/SAM_1894_JPG.rf.0a4039f63042ec889e1e2192a0499576.jpg\n",
      "Contents of yolov3-tiny-rumex.cfg:\n",
      "[net]\n",
      "# Testing\n",
      "batch=64\n",
      "subdivisions=16\n",
      "# Training\n",
      "# batch=64\n",
      "# subdivisions=2\n",
      "width=416\n",
      "height=416\n",
      "channels=3\n",
      "momentum=0.9\n",
      "decay=0.0005\n",
      "angle=0\n",
      "saturation = 1.5\n",
      "exposure = 1.5\n",
      "hue=.1\n",
      "\n",
      "learning_rate=0.001\n",
      "burn_in=1000\n",
      "max_batches = 4000\n",
      "Contents of rumex.data:\n",
      "classes = 1\n",
      "train = data/train.txt\n",
      "valid = data/valid.txt\n",
      "names = data/rumex.names\n",
      "backup = backup/\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1930_JPG.rf.b6b74f0e63471c747454a86353f25616_aug_82266c277f4f4398a45131d486d3f88e.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1865_JPG.rf.8ece69aa430394f115452132800a61a7_aug_f3d5310474874f6baf593a70e934f0c0.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1912_JPG.rf.e4d8dea398b6cc5bbd5bd4e21104bde4_aug_2d55bebee4b34695ae02543a42ec7e01.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1889_JPG.rf.ee5ca460036b7bee736500fe3c20376b_aug_028e02ccaa8247cc91353695b9c8077b.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1891_JPG.rf.490e2283e65df6c443c6acb5f3264a15_aug_f08c82483efb458a8371109aa8616aee.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Contents of train.txt:\")\n",
    "!head -n 5 data/train.txt\n",
    "\n",
    "print(\"\\nContents of valid.txt:\")\n",
    "!head -n 5 data/valid.txt\n",
    "\n",
    "\n",
    "print(\"Contents of yolov3-tiny-rumex.cfg:\")\n",
    "!head -n 20 cfg/yolov3-tiny-rumex.cfg\n",
    "\n",
    "\n",
    "print(\"Contents of rumex.data:\")\n",
    "!cat data/rumex.data\n",
    "\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "with open('data/train.txt', 'r') as f:\n",
    "    image_paths = f.readlines()\n",
    "\n",
    "for _ in range(5):  # Test 5 random images\n",
    "    path = random.choice(image_paths).strip()\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load: {path}\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1864_JPG.rf.d9dd294b2c10b9a4001fb548def1c738_aug_aa9fdf3f3f6f4d8e96435fef6ab48583.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1886_JPG.rf.e2460e1a7eaebd5ea9db9d5c1a285829_aug_3e9a095c50ab48c9bae19d8e35b2a54c.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1941_JPG.rf.6434cfbabb3948979930e9688a963ee9_aug_d6e7565e1ffc41938b36e254a1d4a3ba.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1916_JPG.rf.e0d5f4c1d35d9fbbde544a298688f6c6_aug_e15fd957ac0d4f8aaa7a8dc202738dd0.jpg\n",
      "Successfully loaded: ../data/augmented_dataset/train/SAM_1880_JPG.rf.dfc7414e9efb61f33bbd7d70e965a246_aug_8be0fe1103304aac80f80e2238a5c639.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "\n",
    "with open('data/train.txt', 'r') as f:\n",
    "    image_paths = f.readlines()\n",
    "\n",
    "for _ in range(5):  # Test 5 random images\n",
    "    path = random.choice(image_paths).strip()\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load: {path}\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
